
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/01_encoder.ipynb

from exp.nb_00 import *

import torch.nn.functional as F

def init_cnn(m):
    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
    for l in m.children(): init_cnn(l)

def noop(x): return x

relu = nn.ReLU()

def conv(ni, nf, ks=3, stride=1, padding=1, **kwargs):
    _conv = nn.Conv2d(ni, nf, kernel_size=ks,stride=stride,padding=padding, **kwargs)
    nn.init.kaiming_normal_(_conv.weight)
    nn.init.zeros_(_conv.bias)
    return _conv

def get_stats(w):
    return w.mean(), w.std()

class ResBlock(nn.Module):
    def __init__(self, ni, nh=128):
        super().__init__()

        self.conv1 = conv(ni, nh)
        self.conv2 = conv(nh, ni)
        #initilize 2nd conv with zeros to preserve variance
        nn.init.zeros_(self.conv2.weight)
        nn.init.zeros_(self.conv2.bias)

    def forward(self, x):
        return x  + self.conv2(F.relu(self.conv1(x)))

def children(m): return list(m.children())

class Hook():
    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
    def remove(self): self.hook.remove()
    def __del__(self): self.remove()

class Binarizer(torch.autograd.Function):
    @staticmethod
    def forward(ctx, i):
        if i.is_cuda: return torch.where(i > 0.5, tensor(1.).cuda(), tensor(0.).cuda())
        return torch.where(i > 0.5, tensor(1.), tensor(0.))

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output

def bin_values(x):
    return Binarizer.apply(x)

class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x): return self.func(x)

class Binarizer(torch.autograd.Function):
    @staticmethod
    def forward(ctx, i):
        return (i>0.5).float()

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output

def bin_values(x):
    return Binarizer.apply(x)

class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x): return self.func(x)

class Encoder(nn.Module):
    def __init__(self,return_imp_map=False):
        super(Encoder, self).__init__()
        self.return_imp_map = return_imp_map
        self.stem = nn.Sequential(conv(3, 128, 8, 4, 2), relu,
                                   ResBlock(128), relu,
                                   conv(128, 256, 4, 2, 1), relu,
                                   ResBlock(256), relu,
                                   ResBlock(256), relu)

        self.head = nn.Sequential(conv(256, 64, 3, 1, 1),
                                   nn.Sigmoid(),
                                   Lambda(bin_values))


        self.imp_map_extractor = nn.Sequential(conv(256,128), relu,
                                                conv(128,128), relu,
                                                conv(128,1), nn.Sigmoid())

        #initiating layers before Sigmoid with Xavier
        nn.init.xavier_normal_(self.head[0].weight)
        nn.init.xavier_normal_(self.imp_map_extractor[4].weight)

    def extra_repr(self):
        params = sum(p.numel() for p in self.parameters())
        return f'Total Params: {params}'

    def forward(self,x):
        stem = self.stem(x)
        if self.return_imp_map:return self.head(stem), self.imp_map_extractor(stem)
        else: return self.head(stem)